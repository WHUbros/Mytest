{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data clean and vectorization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import Row\n",
    "\n",
    "#data cleaning via RDD\n",
    "lines = spark.read.text(\"/home/share/musicrate.txt\").rdd\n",
    "parts = lines.map(lambda row: row.value.split(\";\"))\n",
    "ratingsRDD = parts.map(lambda p: Row(userId=int(p[0]), musicId=int(p[1]),\n",
    "                                     rating=float(p[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+\n",
      "|musicId|rating|userId|\n",
      "+-------+------+------+\n",
      "|  76783|   7.0|   953|\n",
      "|    907|   3.0|   954|\n",
      "|  32555|   5.0|   954|\n",
      "|  33010|   4.0|   954|\n",
      "|   8078|   8.0|   954|\n",
      "|  34709|  10.0|   954|\n",
      "|   3494|   4.0|   954|\n",
      "|   3448|   6.0|   954|\n",
      "|   3453|  10.0|   954|\n",
      "| 344015|   3.0|   954|\n",
      "|  34882|   4.0|   954|\n",
      "|   3418|   5.0|   954|\n",
      "|  33629|   6.0|   954|\n",
      "|  35333|   7.0|   954|\n",
      "|   3171|   6.0|   954|\n",
      "|   3185|   3.0|   954|\n",
      "|   3502|   2.0|   954|\n",
      "|   3214|   6.0|   954|\n",
      "|  35085|   7.0|   954|\n",
      "|   2542|   9.0|   954|\n",
      "+-------+------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings = spark.createDataFrame(ratingsRDD)\n",
    "ratings.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "(training, test) = ratings.randomSplit([0.8, 0.2])\n",
    "als = ALS(maxIter=5, regParam=0.01, implicitPrefs=True,userCol=\"userId\", itemCol=\"musicId\", ratingCol=\"rating\",\n",
    "          coldStartStrategy=\"drop\")\n",
    "\n",
    "#fit training data into model\n",
    "model = als.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(test)\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",\n",
    "                                predictionCol=\"prediction\")\n",
    "#rmse = evaluator.evaluate(predictions)\n",
    "#print(\"Root-mean-square error = \" + str(rmse))\n",
    "# Generate top 10 movie recommendations for each user\n",
    "userRecs = model.recommendForAllUsers(10)\n",
    "# Generate top 10 user recommendations for each movie\n",
    "movieRecs = model.recommendForAllItems(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|userId|recommendations                                                                                                                                                                         |\n",
      "+------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|65    |[[4773,6.349504], [29571,5.619185], [60911,4.6032057], [86970,4.6032057], [87851,4.6032057], [30187,4.6032057], [4627,3.9972641], [8395,3.74232], [70616,3.5844743], [90684,3.5583975]] |\n",
      "|53    |[[8395,5.9981704], [4773,5.591066], [29571,5.019331], [94404,4.704097], [81450,4.116084], [7017,4.116084], [42996,4.02158], [26702,4.02158], [42589,4.02158], [6038,3.7646155]]         |\n",
      "|155   |[[6044,3.3920922], [8456,3.308464], [66970,3.1326377], [55112,2.9776182], [29571,2.783332], [15355,2.7708535], [35838,2.3750176], [1213,2.2064247], [98648,2.197906], [30866,1.8386873]]|\n",
      "+------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "userRecs.show(3,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|musicId|recommendations                                                                                                                                                    |\n",
      "+-------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|64590  |[[964,8.998238], [58,5.5098996], [963,5.0233936], [128,4.585006], [986,3.9047723], [157,3.6550195], [32,3.549104], [9,3.1741037], [56,3.08725], [48,3.034688]]     |\n",
      "|96261  |[[964,7.9984326], [58,4.8976884], [963,4.4652386], [128,4.075561], [986,3.4709086], [157,3.248906], [32,3.1547594], [9,2.8214254], [56,2.7442222], [48,2.6975007]] |\n",
      "|243    |[[157,2.9988499], [40,2.0716865], [32,2.0230718], [51,1.8751148], [123,1.5681485], [48,1.5671962], [52,1.5323517], [9,1.5027455], [139,1.2243179], [964,1.2202379]]|\n",
      "+-------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movieRecs.show(3,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196\t242\t3\t881250949\n",
      "Rating(user=196, product=242, rating=3.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Rating(user=196, product=242, rating=3.0)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.recommendation import Rating, ALS\n",
    "Music = sc.textFile(\"/home/share/ml-100k/u.data\")\n",
    "print Music.first()\n",
    "rawdata = Music.map(lambda x: x.split('\\t'))\n",
    "ratings = rawdata.map(lambda x: Rating(int(x[0]),int(x[1]),float(x[2])))\n",
    "print ratings.first()\n",
    "(training, test) = ratings.randomSplit([0.8, 0.2])\n",
    "training.first()\n",
    "test.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, array('d', [-0.27096691727638245, -0.1675601601600647, 0.07899501919746399, -0.46760717034339905, 0.5735558867454529, -0.007676607929170132, -0.27905896306037903, -0.21912045776844025, 0.3271646797657013, 0.31455808877944946, 0.23333097994327545, -0.25859037041664124, 0.06569837033748627, -0.5230371356010437, -0.22805385291576385, 0.030592046678066254, 0.4542900323867798, -0.07986529916524887, -0.4291997253894806, -0.2339172214269638, 0.10900458693504333, -0.44597259163856506, -0.35188573598861694, 0.23993417620658875, -0.1941814124584198, 0.3616299033164978, 0.3770653009414673, -0.11329542100429535, -0.5014241933822632, 0.07350672036409378, -0.226834237575531, 0.15342538058757782, 0.37904873490333557, -0.1535542607307434, -0.19556409120559692, 0.1466103196144104, -0.11856678873300552, -0.19386553764343262, 0.537994384765625, 0.15694577991962433, 0.5005782842636108, -0.12382523715496063, -0.06841724365949631, 0.021241948008537292, -0.36090102791786194, -0.053855519741773605, 0.2296418398618698, -0.4780285060405731, 0.06496009975671768, -0.12853434681892395]))\n",
      "(1, array('d', [-0.8440567851066589, -0.8964300155639648, -0.19953395426273346, -0.6848208904266357, 0.8055425882339478, -1.170978307723999, 0.30802038311958313, -0.6187177896499634, -0.027026766911149025, -0.43694233894348145, -0.0033302772790193558, -0.7420638203620911, 0.0821724683046341, -1.7556424140930176, 0.8438391089439392, -0.6997691988945007, 0.8461728692054749, 0.7227948307991028, -0.11096354573965073, 0.8797277808189392, 0.9593237638473511, -0.12547487020492554, 1.7837671041488647, 0.1445874571800232, 1.1884695291519165, 1.1954753398895264, 0.08737914264202118, -1.8356118202209473, -0.26371389627456665, 0.5860435962677002, -0.6552878022193909, 0.10862595587968826, -0.30492955446243286, -1.323840618133545, -0.043467260897159576, 0.18197999894618988, 1.3447061777114868, -0.9825198650360107, 1.009662389755249, 1.0812742710113525, 1.0853484869003296, -0.3187310993671417, 0.6191290020942688, -0.023545924574136734, 0.5808348655700684, -0.13737571239471436, 0.05230645835399628, 0.18967513740062714, 0.003739157924428582, 0.014799458906054497]))\n"
     ]
    }
   ],
   "source": [
    "model = ALS.train(training, 50, 5, 0.01)\n",
    "userFeatures = model.userFeatures()\n",
    "movieFeatures = model.productFeatures()\n",
    "print userFeatures.first()\n",
    "print movieFeatures.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.76212127174\n"
     ]
    }
   ],
   "source": [
    "preRating = model.predict(196,242)\n",
    "print preRating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recommend 5 movies to user666：\n",
      "Rating(user=666, product=481, rating=5.575768521731508)\n",
      "Rating(user=666, product=887, rating=5.430141864672151)\n",
      "Rating(user=666, product=512, rating=5.30155208979875)\n",
      "Rating(user=666, product=503, rating=5.266556326039051)\n",
      "Rating(user=666, product=675, rating=5.135932534764814)\n"
     ]
    }
   ],
   "source": [
    "print 'recommend 5 movies to user666：'\n",
    "reccomendmovie = model.recommendProducts(666,5)\n",
    "for i in reccomendmovie:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies666 = ratings.groupBy(lambda x : x.user).mapValues(list).lookup(666)\n",
    "print 'the movies and its ratings that user666 have seen：'\n",
    "for i in sorted(movies666[0],key=lambda x : x.rating,reverse=True):\n",
    "    print ('%d     %f'%(i.product,i.rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error : 0.084207\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "from pyspark.mllib.evaluation import RankingMetrics\n",
    "userMovies = ratings.map(lambda rating:(rating.user,rating.product))\n",
    "predictions = model.predictAll(userMovies).map(lambda rating:((rating.user,rating.product), rating.rating))\n",
    "ratingsAndPredictions = ratings.map(lambda rating:((rating.user,rating.product),rating.rating)).join(predictions)\n",
    "predictedAndTrue = ratingsAndPredictions.map(lambda ((userId,product),(predicted, actual))\n",
    "      :(predicted,actual))\n",
    "regressionMetrics = RegressionMetrics(predictedAndTrue)\n",
    "print (\"Mean Squared Error : %f\"%regressionMetrics.meanSquaredError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3.0, 3.027571662864616), (2.0, 1.921936846348708), (4.0, 4.051456270669071)]\n"
     ]
    }
   ],
   "source": [
    "print predictedAndTrue.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+----------+------+\n",
      "|movieId|rating| timestamp|userId|\n",
      "+-------+------+----------+------+\n",
      "|      2|   3.0|1424380312|     0|\n",
      "|      3|   1.0|1424380312|     0|\n",
      "|      5|   2.0|1424380312|     0|\n",
      "|      9|   4.0|1424380312|     0|\n",
      "|     11|   1.0|1424380312|     0|\n",
      "|     12|   2.0|1424380312|     0|\n",
      "|     15|   1.0|1424380312|     0|\n",
      "|     17|   1.0|1424380312|     0|\n",
      "|     19|   1.0|1424380312|     0|\n",
      "|     21|   1.0|1424380312|     0|\n",
      "|     23|   1.0|1424380312|     0|\n",
      "|     26|   3.0|1424380312|     0|\n",
      "|     27|   1.0|1424380312|     0|\n",
      "|     28|   1.0|1424380312|     0|\n",
      "|     29|   1.0|1424380312|     0|\n",
      "|     30|   1.0|1424380312|     0|\n",
      "|     31|   1.0|1424380312|     0|\n",
      "|     34|   1.0|1424380312|     0|\n",
      "|     37|   1.0|1424380312|     0|\n",
      "|     41|   2.0|1424380312|     0|\n",
      "+-------+------+----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movie = spark.createDataFrame(movieRDD)\n",
    "movie.show()\n",
    "#ratings.select(\"userId\",\"movieId\",\"rating\").filter(ratings.rating>2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### splitting train/test datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "(training, test) = movie.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Build the recommendation model using ALS on the training data\n",
    "# Note we set cold start strategy to 'drop' to ensure we don't get NaN evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build pipeline and model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start pipeline\n",
    "#Build als model and set parameters\n",
    "als = ALS(maxIter=5, regParam=0.01, implicitPrefs=True,userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\",\n",
    "          coldStartStrategy=\"drop\")\n",
    "\n",
    "#fit training data into model\n",
    "model = als.fit(training)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-mean-square error = 1.95357410039\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model by computing the RMSE(root mean square error) on the test data\n",
    "#transform dataframe->model->dataframe\n",
    "predictions = model.transform(test)\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",\n",
    "                                predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root-mean-square error = \" + str(rmse))\n",
    "# Generate top 10 movie recommendations for each user\n",
    "userRecs = model.recommendForAllUsers(10)\n",
    "# Generate top 10 user recommendations for each movie\n",
    "movieRecs = model.recommendForAllItems(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|userId|recommendations                                                                                                                                                |\n",
      "+------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|28    |[[6,1.2390108], [54,1.2185225], [14,1.1332309], [3,1.1179535], [59,1.1144238], [34,1.0909408], [63,1.0676852], [20,1.0673373], [78,1.0354528], [65,1.017024]]  |\n",
      "|26    |[[85,1.1621525], [58,1.1486164], [2,1.1011739], [88,1.0527321], [60,1.0263231], [0,1.0214522], [4,1.0199385], [44,1.0165259], [86,1.0112785], [21,1.0010533]]  |\n",
      "|27    |[[51,1.1527448], [40,1.12884], [43,1.1161973], [44,1.0464054], [86,1.040949], [64,1.0365921], [18,1.0326712], [87,0.9890834], [21,0.98380446], [80,0.94883144]]|\n",
      "|12    |[[2,1.3802042], [94,1.1691777], [51,1.1409394], [22,1.1375544], [53,1.1163259], [79,1.1011509], [91,1.0995436], [50,1.0554874], [70,1.046593], [86,1.0360346]] |\n",
      "|22    |[[37,1.1504526], [40,1.1335913], [47,1.1015403], [99,1.0695362], [44,1.0686933], [62,1.0604683], [81,1.0576527], [98,1.04645], [5,1.039032], [11,1.0375711]]   |\n",
      "+------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "userRecs.show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'6', u'148', u'72', u'35', u'0', u'33.6', u'0.627', u'50', u'1']"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "from pyspark.mllib.classification import SVMWithSGD\n",
    "from pyspark.mllib.classification import NaiveBayes\n",
    "pima = sc.textFile('/home/share/pima.txt')\n",
    "pima_new = pima.map(lambda x: x.split(','))\n",
    "pima_new.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(1.0, [6.0,148.0,72.0,35.0,0.0,33.6,0.627,50.0]),\n",
       " LabeledPoint(0.0, [1.0,85.0,66.0,29.0,0.0,26.6,0.351,31.0]),\n",
       " LabeledPoint(1.0, [8.0,183.0,64.0,0.0,0.0,23.3,0.672,32.0]),\n",
       " LabeledPoint(0.0, [1.0,89.0,66.0,23.0,94.0,28.1,0.167,21.0])]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = pima_new.map(lambda x : x[-1])\n",
    "data= pima_new.map(lambda x : (x[-1],x[0:-1])).map(lambda (x,y):(int(x), [float(i) for i in y])).map(lambda (x,y):LabeledPoint(x,Vectors.dense(y)))\n",
    "data.take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625\n",
      "143\n",
      "(1.0,[6.0,148.0,72.0,35.0,0.0,33.6,0.627,50.0])\n",
      "(1.0,[8.0,183.0,64.0,0.0,0.0,23.3,0.672,32.0])\n"
     ]
    }
   ],
   "source": [
    "splits=data.randomSplit([0.8, 0.2])\n",
    "train=splits[0]\n",
    "test=splits[1]\n",
    "print(train.count())\n",
    "print(test.count())\n",
    "print train.first()\n",
    "print test.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters of LogisticRegression：\n",
      "(weights=[1.40059768505,11.6679310642,-10.6383455648,-2.89298096402,2.83201839661,-0.17941870617,0.0603824100052,0.508403278459], intercept=0.0)\n",
      "parameters of SVM：\n",
      "(weights=[1.39627142554,6.4036379116,-16.1864186146,-4.22649855505,-1.63036394269,-1.91952603871,0.0448259642593,-1.3995807259], intercept=0.0)\n",
      "parameters of naivebayes\n",
      "<pyspark.mllib.classification.NaiveBayesModel object at 0xad1517ac>\n"
     ]
    }
   ],
   "source": [
    "LogReg = LogisticRegressionWithSGD.train(train, 15)\n",
    "SVM = SVMWithSGD.train(train, 15)\n",
    "nb = NaiveBayes.train(train)\n",
    "print 'parameters of LogisticRegression：'\n",
    "print(LogReg)\n",
    "print 'parameters of SVM：'\n",
    "print(SVM)\n",
    "print 'parameters of naivebayes'\n",
    "print (nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction of class of first eight data： [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "real calss of first eight data：          [1, 1, 0, 0, 0, 0, 0, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "predictions = LogReg.predict(test.map(lambda x : x.features))\n",
    "print 'prediction of class of first eight data：',predictions.take(10)\n",
    "print 'real calss of first eight data：         ',test.map(lambda x : x.label).map(lambda x: int(x)).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.660714285714\n"
     ]
    }
   ],
   "source": [
    "lrTotalCorrect = test.map(lambda point : 1 if(lrModel.predict(point.features)==point.label) else 0).sum()\n",
    "lrAccuracy = lrTotalCorrect/(test.count()*1.0)\n",
    "print 'accuracy: %s'%lrAccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction of class of first eight data： [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "real calss of first eight data：          [1, 1, 0, 0, 0, 0, 0, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "predictions = SVM.predict(test.map(lambda x : x.features))\n",
    "print 'prediction of class of first eight data：',predictions.take(10)\n",
    "print 'real calss of first eight data：         ',test.map(lambda x : x.label).map(lambda x: int(x)).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction of class of first eight data： [0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]\n",
      "real calss of first eight data：          [1, 1, 1, 0, 0, 1, 0, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "predictions = nb.predict(test.map(lambda x : x.features))\n",
    "print 'prediction of class of first eight data：',predictions.take(10)\n",
    "print 'real calss of first eight data：         ',test.map(lambda x : x.label).map(lambda x: int(x)).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy of Logistic Regression: 0.660714\n",
      "the accuracy of SVM: 0.666667\n"
     ]
    }
   ],
   "source": [
    "LRacc = test.map(lambda point : 1 if(LogReg.predict(point.features)==point.label) else 0).sum()\n",
    "SVMacc = test.map(lambda point : 1 if(SVM.predict(point.features)==point.label) else 0).sum()\n",
    "LRacc = LRacc/(test.count()*1.0)\n",
    "SVMacc = SVMacc/(test.count()*1.0)\n",
    "print 'the accuracy of Logistic Regression: %f'%LRacc\n",
    "print 'the accuracy of SVM: %f'%SVMacc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy of Naivebayes: 0.690789\n"
     ]
    }
   ],
   "source": [
    "nbacc = test.map(lambda point : 1 if (nb.predict(point.features) == point.label) else 0).sum()\n",
    "nbacc = nbacc/(1.0*test.count())\n",
    "print 'the accuracy of Naivebayes: %f'%nbacc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext   \n",
    "data=sc.textFile(\"/home/share/wiki_00\")  \n",
    "import nltk  \n",
    "from nltk.corpus import stopwords  \n",
    "from functools import reduce  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'{', u'``', u'id', u\"''\", u':', u'``', u'54663483', u\"''\", u',', u'``', u'url', u\"''\", u':', u'``', u'https', u':', u'//en.wikipedia.org/wiki', u'?', u'curid=54663483', u\"''\", u',', u'``', u'title', u\"''\", u':', u'``', u'Jonathan', u'M.', u'Lamb', u\"''\", u',', u'``', u'text', u\"''\", u':', u'``', u'Jonathan', u'M.', u'Lamb\\n\\nJonathan', u'Matthew', u'Lamb', u'(', u'born', u'November', u'8', u',', u'1981', u')', u'is', u'an', u'American', u'entrepreneur', u',', u'economist', u',', u'author', u',', u'and', u'speaker', u'.', u'\\n\\nBorn', u'in', u'Muncie', u',', u'Indiana', u',', u'Lamb', u'is', u'a', u'graduate', u'of', u'Ball', u'State', u'University', u'with', u'degrees', u'in', u'Economics', u',', u'and', u'Risk', u'Management', u',', u'and', u'has', u'an', u'MBA', u'from', u'North', u'Carolina', u'State', u'University.\\n\\nIn', u'2004', u',', u'Lamb', u'received', u'BS', u'degrees', u'from', u'the', u'Miller', u'College', u'of', u'Business', u'in', u'Risk', u'Management', u'and', u'Economics', u'.', u'He', u'then', u'went', u'on', u'to', u'work', u'as', u'a', u'Power', u'Trader', u'for', u'ACES', u'Power', u'Marketing', u'and', u'as', u'a', u'senior', u'real', u'time', u'trader', u'for', u'Progress', u'Energy', u'.', u'Lamb', u'has', u'started', u'and', u'owned', u'several', u'small', u'businesses', u',', u'and', u'currently', u'owns', u'OptoeV', u',', u'Inc.', u',', u'in', u'Muncie', u',', u'Indiana', u',', u'which', u'is', u'a', u'provider', u'of', u'small', u'autonomous', u'self-driving', u'battery', u'powered', u'electric', u'agriculture', u'equipment', u'.', u'Lamb', u'earned', u'an', u'MBA', u'from', u'the', u'Jenkins', u'MBA', u'Program', u'at', u'North', u'Carolina', u'State', u'University', u'where', u'he', u'was', u'a', u'McLauchlan', u'Leadership', u'Fellow.\\n\\nLamb', u'is', u'an', u'economic', u'writer', u',', u'speaker', u',', u'and', u'TV', u'and', u'radio', u'guest', u'.', u'Lamb', u\"'s\", u'first', u'book', u',', u'\\', u\"''\", u'Economics', u'is', u'Like', u'Sex', u':', u'Common', u'Sense', u'Thinking', u'for', u'Better', u'Decisions', u'Through', u'the', u'Taboo', u'Topics', u'of', u'Money', u',', u'Budgets', u',', u'Markets', u',', u'and', u'Trade\\', u\"''\", u'was', u'published', u'by', u'Morgan', u'James', u'Publishing', u'in', u'2017', u',', u'with', u'a', u'full', u'national', u'release', u'in', u'2018.\\n\\nLamb', u'and', u'his', u'wife', u'Mollee', u',', u'live', u'in', u'Muncie', u'Indiana', u'and', u'have', u'two', u'boys.\\n', u\"''\", u'}']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "tokenized_docs = [word_tokenize(doc) for doc in text]\n",
    "print repr(tokenized_docs[0]).decode(\"unicode-escape\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'id', u'54663499', u'url', u'https', u'enwikipediaorgwiki', u'curid54663499', u'title', u'Online', u'discussion', u'platform', u'text', u'Online', u'discussion', u'platformnnAn', u'online', u'discussion', u'platform', u'is', u'an', u'online', u'platform', u'that', u'allows', u'for', u'or', u'is', u'build', u'specifically', u'for', u'online', u'discussion', u'nnIn', u'1979', u'students', u'from', u'Duke', u'University', u'created', u'the', u'first', u'online', u'discussion', u'platform', u'with', u'UsenetnnOnline', u'discussion', u'platforms', u'can', u'engage', u'people', u'in', u'collective', u'reflection', u'and', u'exchanging', u'perspectives', u'and', u'crosscultural', u'understandingnnPublic', u'display', u'of', u'ideas', u'can', u'encourage', u'intersubjective', u'meaning', u'makingnnOnline', u'discussion', u'platforms', u'may', u'be', u'an', u'important', u'structural', u'means', u'for', u'effective', u'largescale', u'participationnnOnline', u'discussion', u'platforms', u'can', u'play', u'a', u'role', u'in', u'education', u'In', u'recent', u'years', u'online', u'discussion', u'platform', u'have', u'become', u'a', u'significant', u'part', u'of', u'not', u'only', u'distance', u'education', u'but', u'also', u'in', u'campusbased', u'settingsnnThe', u'proposed', u'interactive', u'elearning', u'community', u'iELC', u'is', u'a', u'platform', u'that', u'engages', u'physics', u'students', u'in', u'online', u'and', u'classroom', u'learning', u'tasks', u'In', u'brief', u'classroom', u'discussions', u'fundamental', u'physics', u'formulas', u'definitions', u'and', u'concepts', u'are', u'disclosed', u'after', u'which', u'students', u'participate', u'in', u'the', u'iELC', u'form', u'discussion', u'and', u'utilize', u'chat', u'and', u'dialogue', u'tools', u'to', u'improve', u'their', u'understanding', u'of', u'the', u'subject', u'The', u'teacher', u'then', u'discusses', u'selected', u'forum', u'posts', u'in', u'the', u'subsequent', u'classroom', u'sessionnnClassroom', u'online', u'discussion', u'platforms', u'are', u'one', u'type', u'of', u'such', u'platformsnnRose', u'argues', u'that', u'the', u'basic', u'motivation', u'for', u'the', u'development', u'of', u'e–learning', u'platforms', u'is', u'efficiency', u'of', u'scale', u'—', u'teaching', u'more', u'students', u'for', u'less', u'moneynnA', u'study', u'found', u'that', u'learners', u'will', u'enhance', u'the', u'frequencies', u'of', u'course', u'discussion', u'and', u'actively', u'interact', u'with', u'elearning', u'platform', u'when', u'elearning', u'platform', u'integrates', u'the', u'curriculum', u'reward', u'mechanism', u'into', u'learning', u'activitiesnn', u'City', u'townhall', u'includes', u'a', u'participation', u'platform', u'for', u'policymaking', u'in', u'RotterdamnnOnline', u'discussion', u'platforms', u'may', u'be', u'designed', u'and', u'improved', u'to', u'streamline', u'discussions', u'for', u'efficiency', u'usefulness', u'and', u'quality', u'For', u'instance', u'voting', u'targeted', u'notifications', u'user', u'levels', u'gamification', u'subscriptions', u'bots', u'discussion', u'requirements', u'structurization', u'layout', u'sorting', u'linking', u'feedbackmechanisms', u'reputationfeatures', u'demandsignaling', u'features', u'requestingfeatures', u'visual', u'highlighting', u'separation', u'curation', u'tools', u'for', u'realtime', u'collaboration', u'tools', u'for', u'mobilization', u'of', u'humans', u'and', u'resources', u'standardization', u'dataprocessing', u'segmentation', u'summarization', u'moderation', u'timeintervals', u'categorizationtagging', u'rules', u'and', u'indexing', u'can', u'be', u'leveraged', u'in', u'synergy', u'to', u'improve', u'the', u'platformnnIn', u'2013', u'Sarah', u'Perez', u'claimed', u'that', u'the', u'best', u'platform', u'for', u'online', u'discussion', u'does', u'nt', u'yet', u'exist', u'noting', u'that', u'comment', u'sections', u'could', u'be', u'more', u'useful', u'if', u'they', u'showed', u'which', u'comments', u'or', u'shares', u'have', u'resonated', u'and', u'why', u'and', u'which', u'understands', u'who', u'deserves', u'to', u'be', u'heard', u'nnOnline', u'platforms', u'do', u'nt', u'intrinsically', u'guarantee', u'informed', u'citizen', u'input', u'Research', u'demonstrates', u'that', u'such', u'spaces', u'can', u'even', u'undermine', u'deliberative', u'participation', u'when', u'they', u'allow', u'hostile', u'superficial', u'and', u'misinformed', u'content', u'to', u'dominate', u'the', u'conversation', u'see', u'also', u'Internet', u'troll', u'shitposting', u'A', u'necessary', u'mechanism', u'that', u'enables', u'these', u'platforms', u'to', u'yield', u'informed', u'citizen', u'debate', u'and', u'contribution', u'to', u'policy', u'is', u'deliberation', u'It', u'is', u'argued', u'that', u'the', u'challenge', u'lies', u'in', u'creating', u'an', u'online', u'context', u'that', u'does', u'not', u'merely', u'aggregate', u'public', u'input', u'but', u'promotes', u'informed', u'public', u'discussion', u'that', u'may', u'benefit', u'the', u'policymaking', u'processnnOnline', u'citizen', u'communication', u'has', u'been', u'studied', u'for', u'an', u'evaluations', u'of', u'how', u'deliberative', u'their', u'content', u'is', u'and', u'how', u'selective', u'perception', u'and', u'ideological', u'fragmentation', u'play', u'a', u'role', u'in', u'them', u'see', u'also', u'filter', u'bubble', u'nOne', u'subbranch', u'of', u'online', u'deliberation', u'research', u'is', u'dedicated', u'to', u'the', u'development', u'of', u'new', u'platforms', u'that', u'facilitate', u'deliberative', u'experiences', u'that', u'surpass', u'currently', u'available', u'options', u'nn']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation)) #see documentation here: http://docs.python.org/2/library/string.html\n",
    "\n",
    "tokenized_docs_no_punctuation = []\n",
    "\n",
    "for review in tokenized_docs:\n",
    "    new_review = []\n",
    "    for token in review:\n",
    "        new_token = regex.sub(u'', token)\n",
    "        if not new_token == u'':\n",
    "            new_review.append(new_token)\n",
    "    \n",
    "    tokenized_docs_no_punctuation.append(new_review)\n",
    "    \n",
    "\n",
    "print repr(tokenized_docs_no_punctuation[2]).decode(\"unicode-escape\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'id', u'54663796', u'url', u'https', u'enwikipediaorgwiki', u'curid54663796', u'title', u'Kate', u'Manicom', u'text', u'Kate', u'ManicomnnKate', u'Zilpah', u'Manicom', u'11', u'March', u'1893', u'–', u'27', u'October', u'1937', u'British', u'suffragette', u'trade', u'unionistnnBorn', u'St', u'Pancras', u'area', u'London', u'Manicom', u'attended', u'Southfields', u'Girls', u'School', u'From', u'1911', u'active', u'women', u'suffrage', u'movement', u'She', u'associated', u'Sylvia', u'Pankhurst', u'worked', u'East', u'London', u'Federation', u'Suffragettes', u'She', u'also', u'joined', u'Labour', u'Party', u'Workers', u'Union', u'worked', u'organiser', u'1917', u'In', u'position', u'recruited', u'women', u'workers', u'across', u'country', u'played', u'key', u'role', u'strike', u'Pearl', u'Assurance', u'workersnnThe', u'Representation', u'People', u'Act', u'1918', u'enfranchised', u'women', u'thirty', u'Manicom', u'leading', u'campaigner', u'extend', u'franchise', u'women', u'basis', u'applied', u'men', u'When', u'Workers', u'Union', u'became', u'part', u'Transport', u'General', u'Workers', u'Union', u'TGWU', u'became', u'London', u'District', u'Organiser', u'She', u'also', u'served', u'Trades', u'Union', u'Congress', u'Women', u'Advisory', u'Committee', u'Standing', u'Joint', u'Committee', u'Industrial', u'Women', u'Organisations', u'ref', u'Trades', u'Union', u'Congress', u'Obituary', u'Kate', u'Manicom', u'Report', u'Proceedings', u'70th', u'Annual', u'Trades', u'Union', u'Congress', u'p246', u'ref', u'She', u'also', u'attended', u'1921', u'International', u'Congress', u'Working', u'Women', u'delegate', u'International', u'Labour', u'OrganisationnnFrom', u'1924', u'Manicom', u'worked', u'postal', u'clerk', u'although', u'remained', u'active', u'trade', u'union', u'movementn']\n"
     ]
    }
   ],
   "source": [
    "# Cleaning text of stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "tokenized_docs_no_stopwords = []\n",
    "\n",
    "for doc in tokenized_docs_no_punctuation:\n",
    "    new_term_vector = []\n",
    "    for word in doc:\n",
    "        if not word in stopwords.words('english'):\n",
    "            new_term_vector.append(word)\n",
    "    \n",
    "    tokenized_docs_no_stopwords.append(new_term_vector)\n",
    "\n",
    "print repr(tokenized_docs_no_stopwords[20]).decode(\"unicode-escape\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'id', u'54663796', u'url', u'http', u'enwikipediaorgwiki', u'curid54663796', u'titl', u'Kate', u'Manicom', u'text', u'Kate', u'ManicomnnK', u'Zilpah', u'Manicom', u'11', u'March', u'1893', u'–', u'27', u'Octob', u'1937', u'British', u'suffragett', u'trade', u'unionistnnBorn', u'St', u'Pancra', u'area', u'London', u'Manicom', u'attend', u'Southfield', u'Girl', u'School', u'From', u'1911', u'activ', u'women', u'suffrag', u'movement', u'She', u'associ', u'Sylvia', u'Pankhurst', u'work', u'East', u'London', u'Feder', u'Suffragett', u'She', u'also', u'join', u'Labour', u'Parti', u'Worker', u'Union', u'work', u'organis', u'1917', u'In', u'posit', u'recruit', u'women', u'worker', u'across', u'countri', u'play', u'key', u'role', u'strike', u'Pearl', u'Assur', u'workersnnTh', u'Represent', u'Peopl', u'Act', u'1918', u'enfranchis', u'women', u'thirti', u'Manicom', u'lead', u'campaign', u'extend', u'franchis', u'women', u'basi', u'appli', u'men', u'When', u'Worker', u'Union', u'becam', u'part', u'Transport', u'Gener', u'Worker', u'Union', u'TGWU', u'becam', u'London', u'District', u'Organis', u'She', u'also', u'serv', u'Trade', u'Union', u'Congress', u'Women', u'Advisori', u'Committe', u'Stand', u'Joint', u'Committe', u'Industri', u'Women', u'Organis', u'ref', u'Trade', u'Union', u'Congress', u'Obituari', u'Kate', u'Manicom', u'Report', u'Proceed', u'70th', u'Annual', u'Trade', u'Union', u'Congress', u'p246', u'ref', u'She', u'also', u'attend', u'1921', u'Intern', u'Congress', u'Work', u'Women', u'deleg', u'Intern', u'Labour', u'OrganisationnnFrom', u'1924', u'Manicom', u'work', u'postal', u'clerk', u'although', u'remain', u'activ', u'trade', u'union', u'movementn']\n"
     ]
    }
   ],
   "source": [
    "# Stemming and Lemmatizing\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "snowball = SnowballStemmer('english')\n",
    "wordnet = WordNetLemmatizer()\n",
    "\n",
    "preprocessed_docs = []\n",
    "\n",
    "for doc in tokenized_docs_no_stopwords:\n",
    "    final_doc = []\n",
    "    for word in doc:\n",
    "        final_doc.append(porter.stem(word))\n",
    "        #final_doc.append(snowball.stem(word))\n",
    "        #final_doc.append(wordnet.lemmatize(word))\n",
    "    \n",
    "    preprocessed_docs.append(final_doc)\n",
    "\n",
    "print repr(preprocessed_docs[20]).decode(\"unicode-escape\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "893"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preprocessed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u'id 54664620 url http enwikipediaorgwiki curid54664620 titl Charl A OReilli III text Charl A OReilli IIInnCharl A OReilli III American academ He Frank E Buck Professor Manag Stanford Graduat School Busi He coauthor three book number case studi well coeditor fourth booknnCharl A OReilli III graduat Univers Texa El Paso earn bachelor scienc degre chemistri 1965 He earn MBA inform system 1971 PhD organiz behavior industri relat Haa School Busi Univers California BerkeleynnOReilli assist professor UCLA Anderson School Manag 1976 1978 He taught Haa School Busi 1979 1993 becam tenur professor Sinc 1993 Frank E Buck Professor Manag Stanford Graduat School BusinessnnOReilli coauthor three book coeditor fourth book He publish case studi Cisco System IBM Hi first book The Manag Organ Strategi Tactic Analys publish 1989 In 1997 publish second book Win Through Innov A Practic Guid Manag Organiz Chang Renew book highlight import teamwork foster innov Hi third book Hidden Valu How Compani Get Extraordinari Result With Ordinari Peopl publish 2000 Hi fourth book Lead Disrupt How To Solv Innov Dilemma publish 2016nn'\n"
     ]
    }
   ],
   "source": [
    "pre_svm=[]\n",
    "for i in range(len(preprocessed_docs)):\n",
    "    string = ' '.join(preprocessed_docs[i])\n",
    "    pre_svm.append(string)\n",
    "print repr(pre_svm[89]).decode(\"unicode-escape\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.datasets import dump_svmlight_file\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "#y = sentiments\n",
    "#y = np.zeros(len(preprocessed_docs))\n",
    "#i = 0\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "X = vectorizer.fit_transform(pre_svm)\n",
    "transformer = TfidfTransformer()\n",
    "tfidf = transformer.fit_transform(X)\n",
    "#f = open('/home/share/tezheng.txt', 'w')\n",
    "#dump_svmlight_file(tfidf, k, f, zero_based=False)\n",
    "#f.close()\n",
    "weight=tfidf.toarray()\n",
    "#print X   \n",
    "#print repr(vectorizer.get_feature_names()).decode(\"unicode-escape\")\n",
    "#print tfidf.toarray()\n",
    "#print repr(tfidf.toarray()).decode(\"unicode-escape\")\n",
    "#print len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(893, 19714)\n",
      "17534899\n"
     ]
    }
   ],
   "source": [
    "print weight.shape\n",
    "count=0\n",
    "for i in range(weight.shape[0]):\n",
    "    for j in range(weight.shape[1]):\n",
    "        if weight[i,j]==0 :\n",
    "            count+=1\n",
    "print count        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Within Set Sum of Squared Errors = 876.01160769\n"
     ]
    }
   ],
   "source": [
    "wssse = model.computeCost(sc.parallelize(weight))\n",
    "print(\"Within Set Sum of Squared Errors = \" + str(wssse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "model = KMeans.train(sc.parallelize(weight), 2, maxIterations=10, initializationMode=\"random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  1.0|(19714,[557,641,7...|\n",
      "|  1.0|(19714,[700,1025,...|\n",
      "|  1.0|(19714,[551,689,1...|\n",
      "|  1.0|(19714,[111,575,1...|\n",
      "|  1.0|(19714,[621,689,6...|\n",
      "|  1.0|(19714,[127,1029,...|\n",
      "|  1.0|(19714,[26,119,70...|\n",
      "|  1.0|(19714,[99,344,35...|\n",
      "|  1.0|(19714,[456,545,7...|\n",
      "|  1.0|(19714,[1033,2758...|\n",
      "|  1.0|(19714,[99,560,71...|\n",
      "|  1.0|(19714,[1035,2440...|\n",
      "|  1.0|(19714,[278,281,2...|\n",
      "|  0.0|(19714,[719,732,8...|\n",
      "|  1.0|(19714,[1038,5523...|\n",
      "|  1.0|(19714,[28,58,706...|\n",
      "|  1.0|(19714,[55,68,85,...|\n",
      "|  1.0|(19714,[1041,2483...|\n",
      "|  1.0|(19714,[449,547,6...|\n",
      "|  1.0|(19714,[530,544,5...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = spark.read.format(\"libsvm\").load(\"/home/share/tezheng.txt\")\n",
    "data.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|(19714,[23,719,78...|[-0.9354051278974...|[0.28182941903584...|       1.0|\n",
      "|  0.0|(19714,[24,265,60...|[-0.9354051278974...|[0.28182941903584...|       1.0|\n",
      "|  0.0|(19714,[24,603,71...|[-0.9354051278974...|[0.28182941903584...|       1.0|\n",
      "|  0.0|(19714,[53,62,155...|[-0.9354051278974...|[0.28182941903584...|       1.0|\n",
      "|  0.0|(19714,[55,68,530...|[-0.9354051278974...|[0.28182941903584...|       1.0|\n",
      "|  0.0|(19714,[68,599,62...|[-0.9354051278974...|[0.28182941903584...|       1.0|\n",
      "|  0.0|(19714,[68,609,62...|[-0.9354051278974...|[0.28182941903584...|       1.0|\n",
      "|  0.0|(19714,[85,606,15...|[-0.9354051278974...|[0.28182941903584...|       1.0|\n",
      "|  0.0|(19714,[146,398,6...|[-0.9354051278974...|[0.28182941903584...|       1.0|\n",
      "|  0.0|(19714,[398,544,5...|[-0.9354051278974...|[0.28182941903584...|       1.0|\n",
      "|  0.0|(19714,[398,599,6...|[-0.9354051278974...|[0.28182941903584...|       1.0|\n",
      "|  0.0|(19714,[398,603,7...|[-0.9354051278974...|[0.28182941903584...|       1.0|\n",
      "|  0.0|(19714,[398,606,6...|[-0.9354051278974...|[0.28182941903584...|       1.0|\n",
      "|  0.0|(19714,[398,621,7...|[-0.9354051278974...|[0.28182941903584...|       1.0|\n",
      "|  0.0|(19714,[424,1395,...|[-0.9354051278974...|[0.28182941903584...|       1.0|\n",
      "|  0.0|(19714,[527,1359,...|[-0.9354051278974...|[0.28182941903584...|       1.0|\n",
      "|  0.0|(19714,[544,553,8...|[-0.9354051278974...|[0.28182941903584...|       1.0|\n",
      "|  0.0|(19714,[609,622,7...|[-0.9354051278974...|[0.28182941903584...|       1.0|\n",
      "|  0.0|(19714,[719,842,1...|[-0.9354051278974...|[0.28182941903584...|       1.0|\n",
      "|  0.0|(19714,[719,1242,...|[-0.9354051278974...|[0.28182941903584...|       1.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Test set accuracy = 0.714285714286\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "splits = data.randomSplit([0.9, 0.1])\n",
    "train = splits[0]\n",
    "test = splits[1]\n",
    "lr = LogisticRegression(maxIter=15, regParam=0.4, elasticNetParam=0.8)\n",
    "\n",
    "# train the model\n",
    "model = lr.fit(train)\n",
    "\n",
    "# select example rows to display.\n",
    "predictions = model.transform(test)\n",
    "predictions.show()\n",
    "\n",
    "# compute accuracy on the test set\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test set accuracy = \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(893,)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Within Set Sum of Squared Errors = 848.898676479\n",
      "Cluster Centers: \n",
      "[  1.04551420e-04   9.34171539e-05   8.75270184e-05 ...,   1.46681863e-04\n",
      "   2.05221421e-04   9.18971448e-05]\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "''''\n",
    "from pyspark.ml.clustering import KMeans\n",
    "dataset = spark.read.format(\"libsvm\").load(\"/home/share/tezheng.txt\")\n",
    "kmeans = KMeans().setK(10).setSeed(1)\n",
    "model = kmeans.fit(dataset)\n",
    "\n",
    "# Evaluate clustering by computing Within Set Sum of Squared Errors.\n",
    "wssse = model.computeCost(dataset)\n",
    "print(\"Within Set Sum of Squared Errors = \" + str(wssse))\n",
    "\n",
    "# Shows the result.\n",
    "centers = model.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'KMeansModel' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-7fbfa5ec862b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprediction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'KMeansModel' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "prediction=model.predict(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "u\"cannot resolve '`sentiment`' given input columns: [value];;\\n'Project ['sentiment, 'text]\\n+- Relation[value#262] text\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-837f608fefc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHashingTF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIDF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/share/tezheng.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msentenceData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sentiment\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/test/spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/dataframe.pyc\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   1038\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Bob'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m         \"\"\"\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/test/spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/test/spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: u\"cannot resolve '`sentiment`' given input columns: [value];;\\n'Project ['sentiment, 'text]\\n+- Relation[value#262] text\\n\""
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "df = spark.read.text(\"/home/share/tezheng.txt\")\n",
    "sentenceData = df.select(\"sentiment\",\"text\").toDF(\"label\",\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki=sc.textFile('/home/share/tezheng.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'0 558:0.06170331146465009 642:0.0546609463902866 720:0.02651654620043781 744:0.08252079862417232 1025:0.08252079862417232 2174:0.08252079862417232 2349:0.06077333840307329 2621:0.03695350515079095 3116:0.04926642988214861 3128:0.0576565237902821 3252:0.07446749527694212 3379:0.07780991815911048 3566:0.05520143736844081 3782:0.04995006592522476 3808:0.02801455714677534 3853:0.08252079862417232 4008:0.07187491067281829 4027:0.07187491067281829 4091:0.09920644088610379 4314:0.1300914686936369 4936:0.0396616972838975 5024:0.06382160732558809 5510:0.08252079862417232 6406:0.0385543396958262 6604:0.06796562118073285 6653:0.09168262595939893 7289:0.05317571937423406 7322:0.2186437855611464 7326:0.07780991815911048 7422:0.06504573434681844 7554:0.06170331146465009 7606:0.06641419192971193 7615:0.01161846080182349 7644:0.0576565237902821 8074:0.08252079862417232 8206:0.02942974229947002 8554:0.0541444853292854 8974:0.0460911819820552 9126:0.06641419192971193 9373:0.02925053369560278 9719:0.01161846080182349 9815:0.01161846080182349 9923:0.06504573434681844 9956:0.2092698444356409 10294:0.05520143736844081 10367:0.07446749527694212 10435:0.1489349905538842 10944:0.5446694271137734 10946:0.08252079862417232 11064:0.0627142497375168 11239:0.04584131297969946 11314:0.04104832290927619 11681:0.09321528608611279 11776:0.1127285062171921 11890:0.07187491067281829 11918:0.1992425757891358 11958:0.08252079862417232 12212:0.07446749527694212 12365:0.08252079862417232 12386:0.06077333840307329 12458:0.07187491067281829 12582:0.247562395872517 12761:0.03524497077848923 13041:0.07446749527694212 13391:0.09601830918156251 13431:0.0426231528948982 13719:0.08252079862417232 13884:0.1153130475805642 14655:0.1531722705398882 14854:0.04995006592522476 14859:0.05991231783350265 14919:0.0460911819820552 14954:0.0893334634172258 15113:0.0546609463902866 15232:0.0576565237902821 15259:0.03980784337190257 15387:0.0407255671326064 15632:0.1234066229293002 16223:0.08252079862417232 16245:0.05105742351329606 16246:0.06975661481188028 16318:0.04056749039751308 16323:0.07446749527694212 16686:0.09485622095930368 16887:0.1328283838594239 17049:0.04056749039751308 17056:0.1028763125720514 17490:0.08252079862417232 17722:0.01161846080182349 17814:0.05836088858248173 17869:0.06975661481188028 17917:0.03349661374859152 17951:0.01161846080182349 18021:0.06975661481188028 18091:0.05520143736844081 18092:0.155619836318221 18272:0.05576830397835789 18291:0.03393181297063511 18447:0.07213572098801108 18460:0.07780991815911048 18539:0.01161846080182349 19079:0.04831094976136585 19158:0.0496032204430519 19291:0.03495081681883567 19333:0.05520143736844081'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
